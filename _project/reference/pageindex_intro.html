<!DOCTYPE html><html lang="en-us" class="__variable_bc0dcf __variable_a0ef65 scroll-smooth"><head><meta charSet="utf-8"/><link rel="preload" as="font" href="/_next/static/media/110cbef534704ef8-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="preload" as="font" href="/_next/static/media/5c0c2bcbaa4149ca-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="preload" as="font" href="/_next/static/media/7cba1811e3c25a15-s.p.woff2" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/bfa3fa11e5cc260e.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/323fcbdbf57a95fd.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/3f6d4ccb375eb8ca.css" data-precedence="next"/><link rel="preload" href="/_next/static/chunks/webpack-b19bc20e2cd703aa.js" as="script"/><link rel="preload" href="/_next/static/chunks/fd9d1056-36299019a3926c13.js" as="script"/><link rel="preload" href="/_next/static/chunks/596-b5b6c191669c2d19.js" as="script"/><link rel="preload" href="/_next/static/chunks/main-app-da826559bf2478ad.js" as="script"/><link rel="preload" as="script" href="https://analytics.umami.is/script.js"/><title>PageIndex: Next-Generation Vectorless, Reasoning-based RAG | PageIndex</title><meta name="description" content="We present PageIndex, a reasoning-based RAG system that simulates how human experts navigate and extract knowledge from long documents through tree search."/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://pageindex.ai/blog/pageindex-intro"/><link rel="alternate" type="application/rss+xml" href="https://pageindex.ai/feed.xml"/><meta property="og:title" content="PageIndex: Next-Generation Vectorless, Reasoning-based RAG"/><meta property="og:description" content="We present PageIndex, a reasoning-based RAG system that simulates how human experts navigate and extract knowledge from long documents through tree search."/><meta property="og:url" content="https://pageindex.ai/blog/pageindex-intro"/><meta property="og:site_name" content="PageIndex"/><meta property="og:locale" content="en_US"/><meta property="og:image" content="https://pageindex.ai/static/images/blog/blog_pageindex.jpg"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-09-19T00:00:00.000Z"/><meta property="article:modified_time" content="2025-09-19T00:00:00.000Z"/><meta property="article:author" content="Mingtian Zhang"/><meta property="article:author" content="Yu Tang"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="PageIndex: Next-Generation Vectorless, Reasoning-based RAG"/><meta name="twitter:description" content="We present PageIndex, a reasoning-based RAG system that simulates how human experts navigate and extract knowledge from long documents through tree search."/><meta name="twitter:image" content="https://pageindex.ai/static/images/blog/blog_pageindex.jpg"/><meta name="next-size-adjust"/><link rel="apple-touch-icon" sizes="76x76" href="/static/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="32x32" href="/static/favicons/favicon.ico"/><link rel="icon" type="image/png" sizes="16x16" href="/static/favicons/favicon.ico"/><link rel="manifest" href="/static/favicons/site.webmanifest"/><link rel="mask-icon" href="/static/favicons/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#000000"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="#fff"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><script src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><!--$!--><template data-dgst="NEXT_DYNAMIC_NO_SSR_CODE"></template><!--/$--><body class="bg-white text-black antialiased"><script>!function(){var d=document.documentElement,c=d.classList;c.remove('light','dark');d.style.colorScheme = 'light';c.add('light')}()</script><section class="mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-7xl xl:px-0 undefined"><div class="flex min-h-screen flex-col justify-between font-sans"><header class="fixed top-0 left-0 right-0 z-[9999] border-b border-[#eef1f9] bg-white lg:bg-white/80 lg:backdrop-blur-sm"><div class="max-w-[1200px] mx-auto px-0"><div class="flex items-center justify-center py-7 relative"><a aria-label="PageIndex" class="absolute left-6" href="/"><div class="flex items-center"><img alt="Logo" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" class="w-10 h-10" style="color:transparent" srcSet="/_next/image?url=%2Fstatic%2Fimages%2Flogo.png&amp;w=48&amp;q=75 1x, /_next/image?url=%2Fstatic%2Fimages%2Flogo.png&amp;w=96&amp;q=75 2x" src="/_next/image?url=%2Fstatic%2Fimages%2Flogo.png&amp;w=96&amp;q=75"/></div></a><nav class="hidden md:flex items-center gap-4"><div class="relative flex items-center"><a class="inline-flex items-center font-medium text-sm text-[#142132] hover:text-vectifyl transition-colors" href="/">Home</a></div><div class="relative flex items-center"><a target="_blank" rel="noopener noreferrer" href="https://chat.pageindex.ai" class="inline-flex items-center font-medium text-sm text-[#142132] hover:text-vectifyl transition-colors">Platform</a></div><div class="relative flex items-center"><div class="flex items-center"><button class="inline-flex items-center gap-1.5 font-medium text-sm text-[#142132] hover:text-vectifyl transition-colors">Products<svg class="w-3.5 h-3.5 transition-transform duration-200 " fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path></svg></button></div></div><div class="relative flex items-center"><a class="inline-flex items-center font-medium text-sm text-[#142132] hover:text-vectifyl transition-colors" href="/blog">Blog</a></div><style>@keyframes spin{to{transform:rotate(360deg)}}.tf-v1-popup{position:fixed;top:0;left:0;width:100%;height:100%;background:rgba(0,0,0,.75);transition:opacity .25s ease-in-out;z-index:10001;display:flex;align-items:center;justify-content:center}.tf-v1-popup .tf-v1-iframe-wrapper{position:relative;transition:opacity .25s ease-in-out;min-width:360px;min-height:360px}.tf-v1-popup .tf-v1-iframe-wrapper iframe{width:100%;height:100%;border:none;overflow:hidden;border-radius:8px}.tf-v1-popup .tf-v1-close{display:block;padding:0;margin:0;position:absolute;font-size:32px;font-weight:normal;line-height:24px;width:24px;height:24px;text-align:center;text-transform:none;cursor:pointer;opacity:.75;transition:opacity .25s ease-in-out;text-decoration:none;color:#000;top:-34px;right:0;background:none;border:none;border-radius:0}.tf-v1-popup .tf-v1-close:hover{opacity:1}@media(min-width: 481px){.tf-v1-popup .tf-v1-close{color:#fff !important}}.tf-v1-popup .tf-v1-spinner{border:3px solid #aaa;font-size:40px;width:1em;height:1em;border-radius:.5em;box-sizing:border-box;animation:spin 1s linear infinite;border-top-color:#fff;position:absolute;top:50%;left:50%;margin:-20px 0 0 -20px}@media(max-width: 480px){.tf-v1-popup{width:100% !important;height:100% !important}.tf-v1-popup .tf-v1-iframe-wrapper{position:relative;transition:opacity .25s ease-in-out;min-width:100%;min-height:100%}.tf-v1-popup .tf-v1-iframe-wrapper iframe{border-radius:0}.tf-v1-popup .tf-v1-close{display:block;padding:0;margin:0;position:absolute;font-size:32px;font-weight:normal;line-height:24px;width:24px;height:24px;text-align:center;text-transform:none;cursor:pointer;opacity:.75;transition:opacity .25s ease-in-out;text-decoration:none;color:#000;top:6px;right:8px;background:none;border:none;border-radius:0}.tf-v1-popup .tf-v1-close:hover{opacity:1}}@media(max-width: 480px)and (min-width: 481px){.tf-v1-popup .tf-v1-close{color:#fff !important}}</style><button class="inline-flex items-center justify-center font-medium text-sm text-[#142132] hover:text-vectifyl transition-colors cursor-pointer" data-testid="tf-v1-popup">Contact</button></nav><div class="absolute right-6 flex items-center gap-3"><a target="_blank" rel="noopener noreferrer" href="https://chat.pageindex.ai" class="hidden md:flex items-center gap-2 px-2.5 py-1.5 bg-[#f4f9ff] border border-[#a3d1ff] rounded-md hover:bg-[#e8f3ff] transition-colors"><span class="font-semibold text-sm text-[#3c80c4]">Try Now</span><div class="bg-[#3788D8] rounded-md p-1 shadow-[0px_2px_4px_0px_rgba(24,98,207,0.4)]"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M4 5L15 5M15 5V15M15 5L5.30598 14.375" stroke="#ffffff" stroke-width="1.75" stroke-linecap="square"></path></svg></div></a><button aria-label="Toggle Menu" class="sm:hidden"><img src="/static/images/new-homepage/menu-icon.svg" alt="Close menu" class="h-8 w-8 text-gray-900 dark:text-gray-100"/></button><div class="fixed left-0 top-0 z-10 h-full w-full transform opacity-95 dark:opacity-[0.98] bg-white duration-300 ease-in-out dark:bg-gray-950 translate-x-full"><div class="flex justify-end"><button class="mr-8 mt-11 h-8 w-8" aria-label="Toggle Menu"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 dark:text-gray-100"><path fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></button></div><nav class="fixed mt-8 h-full"><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100 transition-all duration-200 ease-in-out hover:text-vectifyl hover:scale-105" href="/">Home</a></div><div class="px-12 py-4"><a target="_blank" rel="noopener noreferrer" href="https://chat.pageindex.ai" class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100 transition-all duration-200 ease-in-out hover:text-vectifyl hover:scale-105">Platform</a><div class="text-sm text-gray-500 dark:text-gray-400 mt-1">Try PageIndex instantly</div></div><div class="px-12 py-4"><a target="_blank" rel="noopener noreferrer" href="https://chat.pageindex.ai" class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100 transition-all duration-200 ease-in-out hover:text-vectifyl hover:scale-105">Chat</a><div class="text-sm text-gray-500 dark:text-gray-400 mt-1">Try PageIndex instantly</div></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100 transition-all duration-200 ease-in-out hover:text-vectifyl hover:scale-105" href="/mcp">MCP</a><div class="text-sm text-gray-500 dark:text-gray-400 mt-1">Connect PageIndex to agents</div></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100 transition-all duration-200 ease-in-out hover:text-vectifyl hover:scale-105" href="/api">API</a><div class="text-sm text-gray-500 dark:text-gray-400 mt-1">Integrate PageIndex into your workflow</div></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100 transition-all duration-200 ease-in-out hover:text-vectifyl hover:scale-105" href="/blog">Blog</a></div></nav></div></div></div></div></header><main class=""><div class="mt-36" style="font-family:var(--font-lora)"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"PageIndex: Next-Generation Vectorless, Reasoning-based RAG","datePublished":"2025-09-19T00:00:00.000Z","dateModified":"2025-09-19T00:00:00.000Z","description":"We present PageIndex, a reasoning-based RAG system that simulates how human experts navigate and extract knowledge from long documents through tree search.","url":"https://pageindex.ai/blog/pageindex-intro","author":[{"@type":"Person","name":"Mingtian Zhang"},{"@type":"Person","name":"Yu Tang"}]}</script><section class="mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-7xl xl:px-0 undefined"><div class="fixed bottom-8 right-8 hidden flex-col gap-3 md:hidden"><button aria-label="Scroll To Top" class="rounded-full bg-gray-200 p-2 text-gray-500 transition-all hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-400 dark:hover:bg-gray-600"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z" clip-rule="evenodd"></path></svg></button></div><article><div class="max-w-7xl mx-auto px-4"><header class="max-w-3xl mx-auto"><div class="space-y-1 border-b border-gray-200 pb-10 text-center dark:border-gray-700"><div><div class="text-3xl font-semibold  tracking-tight text-gray-900 dark:text-gray-100 sm:text-3xl  md:text-4xl ">PageIndex: Next-Generation Vectorless, Reasoning-based RAG</div><div class="mt-4"><dt class="sr-only ">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400 font-sans"><time dateTime="2025-09-19T00:00:00.000Z">Sep 19, 2025</time></dd></div><div class="flex flex-nowrap justify-center items-center mt-8 space-x-16 font-sans"><span class="flex items-center space-x-2"><a target="_blank" rel="noopener noreferrer" href="https://mingtian.ai" class="text-sm font-medium text-gray-900 dark:text-gray-100 hover:underline">Mingtian Zhang</a></span><span class="flex items-center space-x-2"><a target="_blank" rel="noopener noreferrer" href="https://linkedin.com/in/yu-tang-7b982321" class="text-sm font-medium text-gray-900 dark:text-gray-100 hover:underline">Yu Tang</a></span></div></div></div></header><div class="grid-rows-[auto_1fr] divide-y divide-gray-200 pb-8 dark:divide-gray-700 xl:divide-y-0"><div class="divide-y divide-gray-200 dark:divide-gray-700 xl:col-span-3 xl:row-span-2 xl:pb-0"><div class="prose max-w-none pb-8 pt-8 dark:prose-invert"><img src="/static/images/blog/blog_pageindex.jpg" alt="image" style="width:100%;height:auto"/><p>Large Language Models (LLMs) have become powerful engines for document understanding and question answering. However, they are constrained by a fundamental architectural limit — the <strong>context window</strong>, i.e., the maximum number of tokens the model can process at once. Even though recent models support longer contexts, research such as <a target="_blank" rel="noopener noreferrer" href="https://research.trychroma.com/context-rot">Chroma’s <em>context rot</em> study</a> has shown that recall accuracy deteriorates as context length grows. This makes it challenging for LLMs to accurately interpret and reason over long, domain-specific documents such as financial reports or legal filings.</p><p>To mitigate this limitation, <strong>Retrieval-Augmented Generation (RAG)</strong> has emerged as the dominant solution. Instead of passing the entire document into the model, RAG retrieves and feeds only the most relevant text chunks based on the user’s query, thereby optimizing the effective context length. However, conventional <strong>vector-based RAG</strong> methods depend heavily on static semantic similarity and face several key limitations. To address these challenges, we introduce <strong>PageIndex</strong>, a <strong>reasoning-based retrieval framework</strong> that enables LLMs to dynamically navigate document structures and infer which sections are genuinely relevant, rather than merely retrieving text that appears semantically similar.</p><h1 id="the-limitations-of-vector-based-rag"><a href="#the-limitations-of-vector-based-rag" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a><strong>The Limitations of Vector-Based RAG</strong></h1><p>Vector-based RAG relies on <strong>semantic embeddings</strong> and <strong>vector databases</strong> to identify relevant text chunks. In the <strong>preprocessing stage</strong>, the document is first <strong>split</strong> into smaller chunks, then each chunk is <strong>embedded</strong> into a vector space using an embedding model, and the resulting vectors are <strong>stored</strong> in a vector database such as Chroma or Pinecone. During the <strong>query stage</strong>, the user query is <strong>embedded</strong> using the same embedding model, the database is <strong>searched</strong> for semantically similar chunks, and the system <strong>retrieves</strong> the top-k results, which are then used to form the model’s input context. While simple and effective for short texts, vector-based RAG faces several major challenges:</p><ol><li><p><strong>Query and knowledge space mismatch</strong>. Vector retrieval assumes that the <em>most semantically similar</em> text to the query is also the <em>most relevant</em>. But this isn’t always true — queries often express <em>intent</em>, not <em>content</em>.</p></li><li><p><strong>Semantic similarity is not equivalent to relevance</strong>. This is especially problematic in technical or legal documents, where many passages share near-identical semantics but differ in relevance.</p></li><li><p><strong>Hard chunking breaks the semantic integration</strong>. Documents are split into fixed-size chunks (e.g., 512 or 1000 tokens) for embedding. This “hard chunking” often cuts through sentences, paragraphs, or sections, fragmenting context.</p></li><li><p><strong>Cannot integrate chat history</strong>. Each query is treated independently. The retriever doesn’t know what’s been asked or answered before.</p></li><li><p><strong>Hard to deal with in document reference</strong>. Documents often contain references such as “see Appendix G” or “refer to Table 5.3.” Since these references don’t share semantic similarity with the referenced content, traditional RAG misses them unless additional preprocessing (like a knowledge graph) is performed.</p></li></ol><p>Because of these limitations, even advanced systems like <a target="_blank" rel="noopener noreferrer" href="https://x.com/pashmerepat/status/1926717705660375463"><strong>Claude Code</strong></a> have moved away from traditional <strong>vector-based RAG</strong> for code retrieval, achieving superior precision and speed without relying on vector databases (see this <a target="_blank" rel="noopener noreferrer" href="https://rlancemartin.github.io/2025/04/03/vibe-code/">blog post</a>). We believe the same principle applies to <strong>document retrieval</strong>: rather than depending on static embeddings and semantic similarity, LLMs can reason over a structured representation of a document — deciding <em>where</em> to look next, not merely <em>what</em> looks similar. To this end, we introduce <strong>PageIndex</strong>, a reasoning-based RAG framework that overcomes the constraints of vector-based systems and brings the power of agentic retrieval to long-form, structured documents.</p><h1 id="pageindex-reasoning-based-retrieval"><a href="#pageindex-reasoning-based-retrieval" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>PageIndex: Reasoning-based Retrieval</h1><p><strong>PageIndex&#x27;s Reasoning-based RAG</strong> mimics how humans naturally navigate and extract information from long documents. Unlike traditional vector-based methods that rely on static semantic similarity, this approach uses a <em>dynamic, iterative reasoning process</em> to actively decide where to look next based on the evolving context of the question.</p><p>It follows the following iterative process:</p><ol><li><p><strong>Read the Table of Contents (ToC)</strong>. Understand the document&#x27;s structure and identify sections that might be relevant.</p></li><li><p><strong>Select a Section</strong>. Choose the section most likely to contain useful information based on the question.</p></li><li><p><strong>Extract Relevant Information</strong>. Parse the selected section to gather any content that could help answer the question.</p></li><li><p><strong>Is the Information Sufficient?</strong></p><ul><li><strong>Yes →</strong> Proceed to <strong>Answer the Question</strong>.</li><li><strong>No →</strong> Return to <strong>Step 1</strong> and repeat the loop with another section.</li></ul></li><li><p><strong>Answer the Question</strong>. Once enough information is collected, generate a complete and well-supported answer.</p></li></ol><div><img alt="loop" loading="lazy" width="1580" height="592" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=%2Fstatic%2Fimages%2Floop.png&amp;w=1920&amp;q=75 1x, /_next/image?url=%2Fstatic%2Fimages%2Floop.png&amp;w=3840&amp;q=75 2x" src="/_next/image?url=%2Fstatic%2Fimages%2Floop.png&amp;w=3840&amp;q=75"/></div><p>In this process, the ToC serves as a key index for the document, enabling the LLM to navigate and retrieve information efficiently. We discuss how to design an LLM-friendly ToC in the next section.</p><h1 id="table-of-contents--index-for-llms"><a href="#table-of-contents--index-for-llms" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a><strong>&quot;Table of Contents&quot; Index for LLMs</strong></h1><p>We introduce a <strong>JSON-based hierarchical structure</strong> to represent a <strong>Table of Contents (ToC)</strong> for unstructured documents. The ToC acts as an <strong>index tree</strong> that organizes content into hierarchical nodes. Each node represents a logical section (e.g., chapter, paragraph, page) and may contain metadata, a description, and links to its sub-sections.</p><p>This approach allows an LLM to:</p><ul><li>Traverse structured content recursively.</li><li>Retrieve targeted raw data by <code>node_id</code>.</li><li>Associate contextual metadata (e.g., source type, topic, or semantic tags).</li></ul><h3 id="pageindex-tree-index-example-json-format"><a href="#pageindex-tree-index-example-json-format" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a><strong>PageIndex Tree Index Example (JSON Format)</strong></h3><div class="relative"><pre><code class="code-highlight language-python"><span class="code-line">Node <span class="token punctuation">{</span>
</span><span class="code-line">  node_id<span class="token punctuation">:</span> string<span class="token punctuation">,</span>         <span class="token operator">//</span> Unique identifier <span class="token keyword">for</span> this node
</span><span class="code-line">  name<span class="token punctuation">:</span> string<span class="token punctuation">,</span>            <span class="token operator">//</span> Human<span class="token operator">-</span>readable label <span class="token keyword">or</span> title
</span><span class="code-line">  description<span class="token punctuation">:</span> string<span class="token punctuation">,</span>     <span class="token operator">//</span> Optional detailed explanation of the node
</span><span class="code-line">  metadata<span class="token punctuation">:</span> <span class="token builtin">object</span><span class="token punctuation">,</span>        <span class="token operator">//</span> Arbitrary key<span class="token operator">-</span>value pairs <span class="token keyword">for</span> context <span class="token keyword">or</span> attributes
</span><span class="code-line">  sub_nodes<span class="token punctuation">:</span> <span class="token punctuation">[</span>Node<span class="token punctuation">]</span>        <span class="token operator">//</span> Array of child nodes <span class="token punctuation">(</span>recursive structure<span class="token punctuation">)</span>
</span><span class="code-line"><span class="token punctuation">}</span>
</span></code></pre></div><p><strong>Notes:</strong></p><ul><li>The <code>node_id</code> serves as a reference key to locate the corresponding raw data.</li><li>The <code>sub_nodes</code> field allows recursive nesting, forming a full ToC tree.</li><li>The <code>metadata</code> field can store semantic information such as document type, author, timestamp, or relevance scores.</li></ul><p>Each node in the ToC links directly to its corresponding <strong>raw content</strong> (e.g., text, images, tables):</p><div class="relative"><pre><code class="code-highlight language-js"><span class="code-line">node_id → <span class="token function">node_content</span> <span class="token punctuation">(</span>raw content<span class="token punctuation">,</span> extracted text<span class="token punctuation">,</span> images<span class="token punctuation">,</span> etc<span class="token punctuation">.</span><span class="token punctuation">)</span>
</span></code></pre></div><p>This mapping enables the LLM to <strong>select and retrieve</strong> specific nodes as needed, facilitating precise and context-aware information access.</p><p>Unlike a <strong>vector database</strong>, which stores an external, static embeddings index, the <strong>JSON-based ToC index</strong> resides <em>within</em> the LLM’s active reasoning context. We call this an <strong>in-context index</strong> — a structure the model can directly reference, navigate, and reason over during inference. By integrating the index into the model’s context window, the LLM can dynamically decide <em>where to look next</em> rather than depending solely on precomputed similarity scores. This enables in-context <strong>reasoning-driven retrieval</strong>, effectively addressing many of the constraints inherent in traditional vector-based RAG systems.</p><h1 id="overcoming-the-limitations"><a href="#overcoming-the-limitations" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>Overcoming the Limitations</h1><h3 id="1-queryknowledge-space-mismatch"><a href="#1-queryknowledge-space-mismatch" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a><strong>1. Query–Knowledge Space Mismatch</strong></h3><p>Instead of relying solely on embedding overlap, the LLM <strong>uses reasoning to infer which section is likely to contain the answer</strong>. It can “think” about document structure — e.g., <em>“Debt trends are usually in the financial summary section or Appendix G — let’s look there.”</em> This dynamic inference bridges the gap between <em>query meaning</em> and <em>information location</em>.</p><h3 id="2-semantic-similarity--true-relevance"><a href="#2-semantic-similarity--true-relevance" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a><strong>2. Semantic Similarity ≠ True Relevance</strong></h3><p>Reasoning-based retrieval emphasizes <strong>contextual relevance</strong>, not just similarity. The model reads the Table of Contents (ToC) or PageIndex structure, interprets the query’s intent, and <strong>navigates</strong> to sections that actually contain the answer — even if their language is different. This mirrors how humans find answers: by <em>understanding</em> the question, not just matching words.</p><h3 id="3-hard-chunking-breaks-semantic-integration"><a href="#3-hard-chunking-breaks-semantic-integration" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a><strong>3. Hard Chunking Breaks Semantic Integration</strong></h3><p>Rather than chunking arbitrarily, reasoning-based RAG retrieves <strong>semantically coherent sections</strong> (e.g., full pages, sections, or chapters). If the model detects that a section is incomplete, it <strong>iteratively fetches neighboring sections</strong> (e.g., next page or sub-node) until context is sufficient. This preserves logical continuity and minimizes hallucination.</p><h3 id="4-inability-to-integrate-chat-history"><a href="#4-inability-to-integrate-chat-history" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a><strong>4. Inability to Integrate Chat History</strong></h3><p>Retrieval is <strong>context-aware</strong> — the model uses prior conversation history to refine its understanding of the current question. For instance, if the user previously asked about “financial assets,” and now asks, “What about liabilities?”, the retriever knows to explore <em>the same report section</em> under liabilities. This enables coherent, multi-step exploration across multiple turns.</p><h3 id="5-poor-handling-of-in-document-references"><a href="#5-poor-handling-of-in-document-references" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a><strong>5. Poor Handling of In-Document References</strong></h3><p>By leveraging the <strong>PageIndex</strong> or <strong>ToC-based hierarchical structure</strong>, reasoning-based retrieval can <em>follow references like a human reader</em>. When it encounters a phrase like <em>“see Appendix G”</em>, the LLM navigates the index tree to that section and retrieves the relevant data. This allows accurate cross-referencing without manual link-building.</p><p>In the <a target="_blank" rel="noopener noreferrer" href="https://claude.ai/share/b29bdfac-f22a-478d-ac1e-00e604c4f3fd">PageIndex MCP example</a>, the query asked for the <em>total value of deferred assets</em>. The main section (pages 75–82) only reported the <em>increase</em> in value, not the total. On page 77, the text read:</p><blockquote><p>Table 5.3 summarizes the income, expenses, and distributions of the Reserve Banks for 2023 and 2022. Appendix G of this report, ‘Statistical Tables,’ provides more detailed information…</p></blockquote><p>The reasoning-based retriever followed this cue to Appendix G, found the correct table, and returned the total deferred asset value — a task the vector-based retriever would likely fail.</p><h2 id="summary-vector-vs-reasoning-based-rag"><a href="#summary-vector-vs-reasoning-based-rag" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a><strong>Summary: Vector vs. Reasoning-Based RAG</strong></h2><table><thead><tr><th>Limitation</th><th><strong>Vector-Based RAG</strong></th><th><strong>Reasoning-Based RAG</strong></th></tr></thead><tbody><tr><td><strong>1. Query–Knowledge Mismatch</strong></td><td>Matches surface-level similarity; often misses true context</td><td>Uses inference to identify the most relevant document sections</td></tr><tr><td><strong>2. Semantic ≠ Relevance</strong></td><td>Retrieves semantically similar but irrelevant chunks</td><td>Retrieves contextually relevant information</td></tr><tr><td><strong>3. Hard Chunking</strong></td><td>Fixed-length chunks fragment meaning</td><td>Retrieves coherent sections dynamically</td></tr><tr><td><strong>4. No Chat Context</strong></td><td>Each query is isolated</td><td>Multi-turn reasoning considers prior context</td></tr><tr><td><strong>5. Cross-References</strong></td><td>Fails to follow internal document links</td><td>Follows in-text references via ToC/PageIndex reasoning</td></tr></tbody></table><h1 id="conclusion"><a href="#conclusion" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a><strong>Conclusion</strong></h1><p>Vector-based RAG <strong>searches</strong> for similar text whereas reasoning-based RAG <strong>thinks</strong> about <em>where</em> to look and <em>why</em>. By combining structured document representations (like ToC Trees) with iterative reasoning, reasoning-based RAG enables LLMs to retrieve <strong>the relevant information</strong>, not just <strong>similar information</strong> — paving the way for a new generation of intelligent document understanding systems.</p><p>Try <a target="_blank" rel="noopener noreferrer" href="https://chat.pageindex.ai">PageIndex</a> Now.</p></div></div><footer><div class="flex justify-center pt-4 xl:pt-8"><a class="text-gray-500 hover:text-gray-700 dark:text-gray-400 dark:hover:text-gray-300 flex items-center gap-2 hover:underline" href="/blog"><span>Back to Blog</span></a></div></footer></div></div></article><!--$!--><template data-dgst="NEXT_DYNAMIC_NO_SSR_CODE"></template><!--/$--></section></div></main><footer class="relative w-[100vw] max-w-[calc(100vw)] left-1/2 -translate-x-1/2 bg-[#0c121b] overflow-hidden"><div class="absolute inset-0 overflow-hidden opacity-[0.12]"><svg class="w-full h-full" xmlns="http://www.w3.org/2000/svg"><defs><pattern id="footer-grid" width="87.055" height="50.265" patternUnits="userSpaceOnUse"><line x1="0" y1="25.1325" x2="43.5275" y2="0" stroke="#e6ebf3" stroke-width="0.6"></line><line x1="43.5275" y1="50.265" x2="87.055" y2="25.1325" stroke="#e6ebf3" stroke-width="0.6"></line><line x1="0" y1="25.1325" x2="43.5275" y2="50.265" stroke="#e6ebf3" stroke-width="0.6"></line><line x1="43.5275" y1="0" x2="87.055" y2="25.1325" stroke="#e6ebf3" stroke-width="0.6"></line></pattern></defs><rect width="100%" height="100%" fill="url(#footer-grid)"></rect></svg><div class="absolute left-1/2 top-0 -translate-x-1/2 w-[1440px] h-[669px] pointer-events-none" style="background:radial-gradient(ellipse 740px 396px at 50% 44%, rgba(12,18,27,0) 0%, rgba(12,18,27,1) 100%)"></div></div><div class="relative py-12 lg:py-16 px-4 sm:px-6"><div class="max-w-[1200px] mx-auto"><div class="grid grid-cols-1 lg:grid-cols-2 gap-x-12 gap-y-12 lg:gap-y-16 mb-12 lg:mb-16"><div class="flex flex-col justify-between lg:pl-[200px] items-center lg:items-start"><div class="flex flex-col w-full max-w-[330px] gap-2 mb-6"><p class="text-[12px] font-semibold text-[#a1a8bc] uppercase leading-[1.15] tracking-tight">Ready to integrate</p><h2 class="text-[24px] lg:text-[28px] font-semibold text-white leading-[1.12]">Reasoning-based RAG with PageIndex?</h2></div><div class="flex flex-col gap-3 w-[330px]"><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-2 bg-gradient-to-b from-[#61acf6] to-[#3788d8] rounded-[8px] pl-[10px] pr-1 py-1 shadow-[0px_1.5px_3.5px_0px_rgba(28,77,150,0.24)] hover:shadow-lg transition-shadow relative w-full lg:w-auto justify-center h-10" href="https://chat.pageindex.ai"><span class="text-[16px] font-semibold text-white leading-[1.25]" style="text-shadow:rgba(0,98,212,0.24) 0px 2px 6px">Try Now</span><div class="absolute inset-0 pointer-events-none rounded-[inherit] shadow-[0px_-1.5px_3px_1px_inset_rgba(16,82,159,0.12),0px_0.8px_2px_0px_inset_#d0e4ff]"></div></a><style>@keyframes spin{to{transform:rotate(360deg)}}.tf-v1-popup{position:fixed;top:0;left:0;width:100%;height:100%;background:rgba(0,0,0,.75);transition:opacity .25s ease-in-out;z-index:10001;display:flex;align-items:center;justify-content:center}.tf-v1-popup .tf-v1-iframe-wrapper{position:relative;transition:opacity .25s ease-in-out;min-width:360px;min-height:360px}.tf-v1-popup .tf-v1-iframe-wrapper iframe{width:100%;height:100%;border:none;overflow:hidden;border-radius:8px}.tf-v1-popup .tf-v1-close{display:block;padding:0;margin:0;position:absolute;font-size:32px;font-weight:normal;line-height:24px;width:24px;height:24px;text-align:center;text-transform:none;cursor:pointer;opacity:.75;transition:opacity .25s ease-in-out;text-decoration:none;color:#000;top:-34px;right:0;background:none;border:none;border-radius:0}.tf-v1-popup .tf-v1-close:hover{opacity:1}@media(min-width: 481px){.tf-v1-popup .tf-v1-close{color:#fff !important}}.tf-v1-popup .tf-v1-spinner{border:3px solid #aaa;font-size:40px;width:1em;height:1em;border-radius:.5em;box-sizing:border-box;animation:spin 1s linear infinite;border-top-color:#fff;position:absolute;top:50%;left:50%;margin:-20px 0 0 -20px}@media(max-width: 480px){.tf-v1-popup{width:100% !important;height:100% !important}.tf-v1-popup .tf-v1-iframe-wrapper{position:relative;transition:opacity .25s ease-in-out;min-width:100%;min-height:100%}.tf-v1-popup .tf-v1-iframe-wrapper iframe{border-radius:0}.tf-v1-popup .tf-v1-close{display:block;padding:0;margin:0;position:absolute;font-size:32px;font-weight:normal;line-height:24px;width:24px;height:24px;text-align:center;text-transform:none;cursor:pointer;opacity:.75;transition:opacity .25s ease-in-out;text-decoration:none;color:#000;top:6px;right:8px;background:none;border:none;border-radius:0}.tf-v1-popup .tf-v1-close:hover{opacity:1}}@media(max-width: 480px)and (min-width: 481px){.tf-v1-popup .tf-v1-close{color:#fff !important}}</style><button class="inline-flex items-center gap-3 bg-[#f4f9ff] border border-[#daecff] rounded-[8px] pl-[10px] pr-[5px] py-1 hover:bg-white transition-colors w-full lg:w-auto justify-center h-10" data-testid="tf-v1-popup"><span class="text-[16px] font-semibold text-[#142132] leading-[1.25]">Contact us</span></button></div></div><div class="flex flex-col justify-between items-center lg:items-start"><div class="mt-10 mb-6 lg:mb-2 text-center lg:text-left"><h3 class="text-[24px] lg:text-[28px] font-semibold text-white leading-[1.12] mb-2">Subscribe to our newsletter</h3><p class="text-[14px] font-medium text-[#eef1f9] leading-[1.4]">Don&#x27;t miss out on the latest news, features, and blog posts.</p></div><div class="jsx-4de236735e0b6e33 footer-newsletter"><div class="text-center z-1 max-w-[400px]"><div class="pb-1 text-lg font-semibold text-vectifyl"></div><form class="flex flex-col sm:flex-row"><div><label for="email-input"><span class="sr-only">Email address</span><input autoComplete="email" class="focus:ring-primary-300 w-72 rounded-md px-4 focus:border-transparent focus:outline-none focus:ring-2 dark:bg-black" id="email-input" placeholder="Enter your email" required="" type="email" name="email"/></label></div><div class="mt-2 flex w-full rounded-md shadow-sm sm:mt-0 sm:ml-3"><button class="bg-vectifyl w-full rounded-md py-2 px-4 font-medium text-white sm:py-0 hover:bg-vectify dark:hover:bg-vectify focus:ring-primary-600 focus:outline-none focus:ring-2 focus:ring-offset-2 dark:ring-offset-black" type="submit">Subscribe</button></div></form></div></div></div><div class="flex flex-row justify-center lg:justify-start gap-12 sm:gap-16 lg:gap-24 lg:pl-[200px]"><div><h4 class="text-[14px] font-semibold text-[#a1a8bc] leading-[1.25] mb-4 lg:mb-8">Products</h4><ul class="space-y-2 lg:space-y-4"><li><a href="https://chat.pageindex.ai" class="text-[16px] font-medium text-white leading-[1.4] hover:text-[#3788d8] transition-colors">Chat</a></li><li><a href="/mcp" class="text-[16px] font-medium text-white leading-[1.4] hover:text-[#3788d8] transition-colors">MCP</a></li><li><a href="/api" class="text-[16px] font-medium text-white leading-[1.4] hover:text-[#3788d8] transition-colors">API</a></li></ul></div><div><h4 class="text-[14px] font-semibold text-[#a1a8bc] leading-[1.25] mb-4 lg:mb-8">Company</h4><ul class="space-y-2 lg:space-y-4"><li><a href="/blog" class="text-[16px] font-medium text-white leading-[1.4] hover:text-[#3788d8] transition-colors">Blog</a></li><li><a href="/policies" class="text-[16px] font-medium text-white leading-[1.4] hover:text-[#3788d8] transition-colors">Policies</a></li></ul></div><div class="lg:hidden"><h4 class="text-[14px] font-semibold text-[#a1a8bc] leading-[1.25] mb-4">Socials</h4><ul class="space-y-2"><li><a href="https://twitter.com/VectifyAI" class="text-[16px] font-medium text-white leading-[1.4] hover:text-[#3788d8] transition-colors" target="_blank" rel="noopener noreferrer">X (Twitter)</a></li><li><a href="https://github.com/VectifyAI" class="text-[16px] font-medium text-white leading-[1.4] hover:text-[#3788d8] transition-colors" target="_blank" rel="noopener noreferrer">GitHub</a></li><li><a href="https://www.linkedin.com/company/vectify-ai/" class="text-[16px] font-medium text-white leading-[1.4] hover:text-[#3788d8] transition-colors" target="_blank" rel="noopener noreferrer">LinkedIn</a></li><li><a href="https://discord.gg/MgE7JfQH" class="text-[16px] font-medium text-white leading-[1.4] hover:text-[#3788d8] transition-colors" target="_blank" rel="noopener noreferrer">Discord</a></li></ul></div></div><div class="hidden lg:block"><div class="flex flex-row items-end gap-6 mb-8"><h4 class="text-[14px] font-semibold text-[#a1a8bc] leading-[1.25]">Socials</h4><div class="w-px h-[22px] bg-white opacity-40"></div><div class="flex flex-wrap gap-6"><a href="https://twitter.com/VectifyAI" class="text-[16px] font-medium text-white leading-[1.4] hover:text-[#3788d8] transition-colors" target="_blank" rel="noopener noreferrer">X (Twitter)</a><a href="https://github.com/VectifyAI" class="text-[16px] font-medium text-white leading-[1.4] hover:text-[#3788d8] transition-colors" target="_blank" rel="noopener noreferrer">GitHub</a><a href="https://www.linkedin.com/company/vectify-ai/" class="text-[16px] font-medium text-white leading-[1.4] hover:text-[#3788d8] transition-colors" target="_blank" rel="noopener noreferrer">LinkedIn</a><a href="https://discord.gg/MgE7JfQH" class="text-[16px] font-medium text-white leading-[1.4] hover:text-[#3788d8] transition-colors" target="_blank" rel="noopener noreferrer">Discord</a></div></div><button class="inline-flex items-center gap-3 bg-gradient-to-b from-[#142132] to-[#0c121b] border border-[rgba(255,255,255,0.18)] rounded-[8px] pl-2 pr-4 py-1.5 hover:border-[rgba(255,255,255,0.3)] transition-colors text-left"><div class="w-[29px] h-[29px] rounded-full bg-[#2e3a51] flex items-center justify-center"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" class="w-4 h-4 text-white ml-0.5"><path stroke-linecap="round" stroke-linejoin="round" d="M5.25 5.653c0-.856.917-1.398 1.667-.986l11.54 6.348a1.125 1.125 0 010 1.971l-11.54 6.347a1.125 1.125 0 01-1.667-.985V5.653z"></path></svg></div><div class="flex flex-col gap-0.5"><p class="text-[10px] font-medium text-[#a1a8bc] leading-[1.15] capitalize">Introduction</p><p class="text-[14px] font-semibold text-white leading-[1.25]">Watch video</p></div></button></div><div class="lg:hidden flex justify-center"><button class="inline-flex items-center gap-3 bg-gradient-to-b from-[#142132] to-[#0c121b] border border-[rgba(255,255,255,0.18)] rounded-[8px] pl-2 pr-4 py-1.5 hover:border-[rgba(255,255,255,0.3)] transition-colors text-left"><div class="w-[29px] h-[29px] rounded-full bg-[#2e3a51] flex items-center justify-center"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" class="w-4 h-4 text-white ml-0.5"><path stroke-linecap="round" stroke-linejoin="round" d="M5.25 5.653c0-.856.917-1.398 1.667-.986l11.54 6.348a1.125 1.125 0 010 1.971l-11.54 6.347a1.125 1.125 0 01-1.667-.985V5.653z"></path></svg></div><div class="flex flex-col gap-0.5"><p class="text-[10px] font-medium text-[#a1a8bc] leading-[1.15] capitalize">Introduction</p><p class="text-[14px] font-semibold text-white leading-[1.25]">Watch video</p></div></button></div></div><div class="flex justify-center text-center"><p class="text-[14px] lg:text-[16px] font-medium text-white leading-[1.25]">© 2025 Vectify AI. All rights reserved.</p></div></div></div></footer></div></section><script src="/_next/static/chunks/webpack-b19bc20e2cd703aa.js" async=""></script><script src="/_next/static/chunks/fd9d1056-36299019a3926c13.js" async=""></script><script src="/_next/static/chunks/596-b5b6c191669c2d19.js" async=""></script><script src="/_next/static/chunks/main-app-da826559bf2478ad.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/110cbef534704ef8-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/media/5c0c2bcbaa4149ca-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n3:HL[\"/_next/static/media/7cba1811e3c25a15-s.p.woff2\",{\"as\":\"font\",\"type\":\"font/woff2\"}]\n4:HL[\"/_next/static/css/bfa3fa11e5cc260e.css\",{\"as\":\"style\"}]\n5:HL[\"/_next/static/css/323fcbdbf57a95fd.css\",{\"as\":\"style\"}]\n0:\"$L6\"\n"])</script><script>self.__next_f.push([1,"7:HL[\"/_next/static/css/3f6d4ccb375eb8ca.css\",{\"as\":\"style\"}]\n"])</script><script>self.__next_f.push([1,"8:I{\"id\":57948,\"chunks\":[\"272:static/chunks/webpack-b19bc20e2cd703aa.js\",\"971:static/chunks/fd9d1056-36299019a3926c13.js\",\"596:static/chunks/596-b5b6c191669c2d19.js\"],\"name\":\"default\",\"async\":false}\na:I{\"id\":56628,\"chunks\":[\"272:static/chunks/webpack-b19bc20e2cd703aa.js\",\"971:static/chunks/fd9d1056-36299019a3926c13.js\",\"596:static/chunks/596-b5b6c191669c2d19.js\"],\"name\":\"GlobalError\",\"async\":false}\nb:I{\"id\":25572,\"chunks\":[\"685:static/chunks/685-81bd5f2d35263f6b.js\",\"763:static/chunks/763-0ee86c963b6965f8.j"])</script><script>self.__next_f.push([1,"s\",\"64:static/chunks/64-893159eb9eb2ae2b.js\",\"977:static/chunks/977-6800e0aaf9fef245.js\",\"185:static/chunks/app/layout-33d1bff7266b90d1.js\"],\"name\":\"Analytics\",\"async\":false}\nc:I{\"id\":8464,\"chunks\":[\"685:static/chunks/685-81bd5f2d35263f6b.js\",\"763:static/chunks/763-0ee86c963b6965f8.js\",\"64:static/chunks/64-893159eb9eb2ae2b.js\",\"977:static/chunks/977-6800e0aaf9fef245.js\",\"185:static/chunks/app/layout-33d1bff7266b90d1.js\"],\"name\":\"ThemeProviders\",\"async\":false}\nd:I{\"id\":83015,\"chunks\":[\"685:static/chunks/685-"])</script><script>self.__next_f.push([1,"81bd5f2d35263f6b.js\",\"763:static/chunks/763-0ee86c963b6965f8.js\",\"64:static/chunks/64-893159eb9eb2ae2b.js\",\"977:static/chunks/977-6800e0aaf9fef245.js\",\"185:static/chunks/app/layout-33d1bff7266b90d1.js\"],\"name\":\"\",\"async\":false}\ne:I{\"id\":44633,\"chunks\":[\"685:static/chunks/685-81bd5f2d35263f6b.js\",\"763:static/chunks/763-0ee86c963b6965f8.js\",\"629:static/chunks/629-97b84782b205ec2f.js\",\"797:static/chunks/app/blog/[...slug]/page-47417712ae05daf1.js\"],\"name\":\"\",\"async\":false}\nf:I{\"id\":81080,\"chunks\":[\"685:static/"])</script><script>self.__next_f.push([1,"chunks/685-81bd5f2d35263f6b.js\",\"763:static/chunks/763-0ee86c963b6965f8.js\",\"64:static/chunks/64-893159eb9eb2ae2b.js\",\"977:static/chunks/977-6800e0aaf9fef245.js\",\"185:static/chunks/app/layout-33d1bff7266b90d1.js\"],\"name\":\"\",\"async\":false}\n10:I{\"id\":73657,\"chunks\":[\"685:static/chunks/685-81bd5f2d35263f6b.js\",\"763:static/chunks/763-0ee86c963b6965f8.js\",\"64:static/chunks/64-893159eb9eb2ae2b.js\",\"977:static/chunks/977-6800e0aaf9fef245.js\",\"185:static/chunks/app/layout-33d1bff7266b90d1.js\"],\"name\":\"KBarSearchPro"])</script><script>self.__next_f.push([1,"vider\",\"async\":false}\n11:I{\"id\":47767,\"chunks\":[\"272:static/chunks/webpack-b19bc20e2cd703aa.js\",\"971:static/chunks/fd9d1056-36299019a3926c13.js\",\"596:static/chunks/596-b5b6c191669c2d19.js\"],\"name\":\"default\",\"async\":false}\n12:I{\"id\":57920,\"chunks\":[\"272:static/chunks/webpack-b19bc20e2cd703aa.js\",\"971:static/chunks/fd9d1056-36299019a3926c13.js\",\"596:static/chunks/596-b5b6c191669c2d19.js\"],\"name\":\"default\",\"async\":false}\n13:I{\"id\":46685,\"chunks\":[\"685:static/chunks/685-81bd5f2d35263f6b.js\",\"763:static/chunks/7"])</script><script>self.__next_f.push([1,"63-0ee86c963b6965f8.js\",\"629:static/chunks/629-97b84782b205ec2f.js\",\"797:static/chunks/app/blog/[...slug]/page-47417712ae05daf1.js\"],\"name\":\"\",\"async\":false}\n15:I{\"id\":71280,\"chunks\":[\"685:static/chunks/685-81bd5f2d35263f6b.js\",\"763:static/chunks/763-0ee86c963b6965f8.js\",\"64:static/chunks/64-893159eb9eb2ae2b.js\",\"977:static/chunks/977-6800e0aaf9fef245.js\",\"185:static/chunks/app/layout-33d1bff7266b90d1.js\"],\"name\":\"\",\"async\":false}\n16:I{\"id\":15069,\"chunks\":[\"685:static/chunks/685-81bd5f2d35263f6b.js\",\"763:st"])</script><script>self.__next_f.push([1,"atic/chunks/763-0ee86c963b6965f8.js\",\"64:static/chunks/64-893159eb9eb2ae2b.js\",\"977:static/chunks/977-6800e0aaf9fef245.js\",\"185:static/chunks/app/layout-33d1bff7266b90d1.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"6:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/bfa3fa11e5cc260e.css\",\"precedence\":\"next\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/323fcbdbf57a95fd.css\",\"precedence\":\"next\"}]],[\"$\",\"$L8\",null,{\"buildId\":\"d2XPp6fbYUwiSMRj9qWl1\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/pageindex-intro\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"pageindex-intro\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"pageindex-intro\\\"]}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":\"$L9\",\"globalErrorComponent\":\"$a\",\"children\":[[\"$\",\"html\",null,{\"lang\":\"en-us\",\"className\":\"__variable_bc0dcf __variable_a0ef65 scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"link\",null,{\"rel\":\"apple-touch-icon\",\"sizes\":\"76x76\",\"href\":\"/static/favicons/favicon.ico\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"sizes\":\"32x32\",\"href\":\"/static/favicons/favicon.ico\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"sizes\":\"16x16\",\"href\":\"/static/favicons/favicon.ico\"}],[\"$\",\"link\",null,{\"rel\":\"manifest\",\"href\":\"/static/favicons/site.webmanifest\"}],[\"$\",\"link\",null,{\"rel\":\"mask-icon\",\"href\":\"/static/favicons/safari-pinned-tab.svg\",\"color\":\"#5bbad5\"}],[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#000000\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: light)\",\"content\":\"#fff\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: dark)\",\"content\":\"#000\"}],[\"$\",\"link\",null,{\"rel\":\"alternate\",\"type\":\"application/rss+xml\",\"href\":\"/feed.xml\"}],[\"$\",\"$Lb\",null,{}],[\"$\",\"body\",null,{\"className\":\"bg-white text-black antialiased\",\"children\":[\"$\",\"$Lc\",null,{\"children\":[[\"$undefined\",\"$undefined\",\"$undefined\",[\"$\",\"$Ld\",null,{\"async\":true,\"defer\":true,\"data-website-id\":\"$undefined\",\"src\":\"https://analytics.umami.is/script.js\"}],\"$undefined\"],[\"$\",\"$Le\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"flex min-h-screen flex-col justify-between font-sans\",\"children\":[[\"$\",\"$Lf\",null,{}],[\"$\",\"main\",null,{\"className\":\"\",\"children\":[\"$\",\"$L10\",null,{\"kbarConfig\":{\"searchDocumentsPath\":\"search.json\"},\"children\":[\"$\",\"$L11\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"template\":[\"$\",\"$L12\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-start justify-start md:flex-row md:items-center md:justify-center md:space-x-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"space-x-2  py-64\",\"children\":[\"$\",\"h1\",null,{\"className\":\"text-6xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 md:border-r-2 md:px-6 md:text-8xl md:leading-14\",\"children\":\"404\"}]}],[\"$\",\"div\",null,{\"className\":\"max-w-md\",\"children\":[[\"$\",\"p\",null,{\"className\":\"mb-4 text-xl font-bold leading-normal md:text-2xl\",\"children\":\"Sorry we couldn't find this page.\"}],[\"$\",\"p\",null,{\"className\":\"mb-8\",\"children\":\"But dont worry, you can find plenty of other things on our homepage.\"}],[\"$\",\"$L13\",null,{\"href\":\"/\",\"className\":\"focus:shadow-outline-blue inline rounded-lg border border-transparent bg-blue-600 px-4 py-2 text-sm font-medium leading-5 text-white shadow transition-colors duration-150 hover:bg-blue-700 focus:outline-none dark:hover:bg-blue-500\",\"children\":\"Back to homepage\"}]]}]]}],\"notFoundStyles\":[],\"childProp\":{\"current\":[\"$\",\"$L11\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"template\":[\"$\",\"$L12\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$\",\"$L11\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",[\"slug\",\"pageindex-intro\",\"c\"],\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"template\":[\"$\",\"$L12\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$L14\",null],\"segment\":\"__PAGE__?{\\\"slug\\\":[\\\"pageindex-intro\\\"]}\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/3f6d4ccb375eb8ca.css\",\"precedence\":\"next\"}]]}],\"segment\":[\"slug\",\"pageindex-intro\",\"c\"]},\"styles\":[]}],\"segment\":\"blog\"},\"styles\":[]}]}]}],[\"$\",\"$L15\",null,{}],[\"$\",\"$L16\",null,{}]]}]}]]}]}]]}],null]}]]\n"])</script><script>self.__next_f.push([1,"17:I{\"id\":12630,\"chunks\":[\"685:static/chunks/685-81bd5f2d35263f6b.js\",\"763:static/chunks/763-0ee86c963b6965f8.js\",\"629:static/chunks/629-97b84782b205ec2f.js\",\"797:static/chunks/app/blog/[...slug]/page-47417712ae05daf1.js\"],\"name\":\"\",\"async\":false}\n18:I{\"id\":63222,\"chunks\":[\"685:static/chunks/685-81bd5f2d35263f6b.js\",\"763:static/chunks/763-0ee86c963b6965f8.js\",\"629:static/chunks/629-97b84782b205ec2f.js\",\"797:static/chunks/app/blog/[...slug]/page-47417712ae05daf1.js\"],\"name\":\"Image\",\"async\":false}\n19:I{\"id\":1"])</script><script>self.__next_f.push([1,"1885,\"chunks\":[\"685:static/chunks/685-81bd5f2d35263f6b.js\",\"763:static/chunks/763-0ee86c963b6965f8.js\",\"629:static/chunks/629-97b84782b205ec2f.js\",\"797:static/chunks/app/blog/[...slug]/page-47417712ae05daf1.js\"],\"name\":\"\",\"async\":false}\n1a:\"$Sreact.suspense\"\n1b:I{\"id\":33699,\"chunks\":[\"685:static/chunks/685-81bd5f2d35263f6b.js\",\"763:static/chunks/763-0ee86c963b6965f8.js\",\"629:static/chunks/629-97b84782b205ec2f.js\",\"797:static/chunks/app/blog/[...slug]/page-47417712ae05daf1.js\"],\"name\":\"NoSSR\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"1c:I{\"id\":56917,\"chunks\":[\"685:static/chunks/685-81bd5f2d35263f6b.js\",\"763:static/chunks/763-0ee86c963b6965f8.js\",\"629:static/chunks/629-97b84782b205ec2f.js\",\"797:static/chunks/app/blog/[...slug]/page-47417712ae05daf1.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"14:[\"$\",\"div\",null,{\"className\":\"mt-36\",\"style\":{\"fontFamily\":\"var(--font-lora)\"},\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BlogPosting\\\",\\\"headline\\\":\\\"PageIndex: Next-Generation Vectorless, Reasoning-based RAG\\\",\\\"datePublished\\\":\\\"2025-09-19T00:00:00.000Z\\\",\\\"dateModified\\\":\\\"2025-09-19T00:00:00.000Z\\\",\\\"description\\\":\\\"We present PageIndex, a reasoning-based RAG system that simulates how human experts navigate and extract knowledge from long documents through tree search.\\\",\\\"url\\\":\\\"https://pageindex.ai/blog/pageindex-intro\\\",\\\"author\\\":[{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"Mingtian Zhang\\\"},{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"Yu Tang\\\"}]}\"}}],[\"$\",\"$Le\",null,{\"children\":[[\"$\",\"$L17\",null,{}],[\"$\",\"article\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-7xl mx-auto px-4\",\"children\":[[\"$\",\"header\",null,{\"className\":\"max-w-3xl mx-auto\",\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-1 border-b border-gray-200 pb-10 text-center dark:border-gray-700\",\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"text-3xl font-semibold  tracking-tight text-gray-900 dark:text-gray-100 sm:text-3xl  md:text-4xl \",\"children\":\"PageIndex: Next-Generation Vectorless, Reasoning-based RAG\"}],[\"$\",\"div\",null,{\"className\":\"mt-4\",\"children\":[[\"$\",\"dt\",null,{\"className\":\"sr-only \",\"children\":\"Published on\"}],[\"$\",\"dd\",null,{\"className\":\"text-base font-medium leading-6 text-gray-500 dark:text-gray-400 font-sans\",\"children\":[\"$\",\"time\",null,{\"dateTime\":\"2025-09-19T00:00:00.000Z\",\"children\":\"Sep 19, 2025\"}]}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-nowrap justify-center items-center mt-8 space-x-16 font-sans\",\"children\":[[\"$\",\"span\",\"Mingtian Zhang\",{\"className\":\"flex items-center space-x-2\",\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://mingtian.ai\",\"className\":\"text-sm font-medium text-gray-900 dark:text-gray-100 hover:underline\",\"children\":\"Mingtian Zhang\"}]}],[\"$\",\"span\",\"Yu Tang\",{\"className\":\"flex items-center space-x-2\",\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://linkedin.com/in/yu-tang-7b982321\",\"className\":\"text-sm font-medium text-gray-900 dark:text-gray-100 hover:underline\",\"children\":\"Yu Tang\"}]}]]}]]}]}]}],[\"$\",\"div\",null,{\"className\":\"grid-rows-[auto_1fr] divide-y divide-gray-200 pb-8 dark:divide-gray-700 xl:divide-y-0\",\"children\":[[\"$\",\"div\",null,{\"className\":\"divide-y divide-gray-200 dark:divide-gray-700 xl:col-span-3 xl:row-span-2 xl:pb-0\",\"children\":[\"$\",\"div\",null,{\"className\":\"prose max-w-none pb-8 pt-8 dark:prose-invert\",\"children\":[[\"$\",\"img\",null,{\"src\":\"/static/images/blog/blog_pageindex.jpg\",\"alt\":\"image\",\"style\":{\"width\":\"100%\",\"height\":\"auto\"}}],[\"$\",\"p\",null,{\"children\":[\"Large Language Models (LLMs) have become powerful engines for document understanding and question answering. However, they are constrained by a fundamental architectural limit — the \",[\"$\",\"strong\",null,{\"children\":\"context window\"}],\", i.e., the maximum number of tokens the model can process at once. Even though recent models support longer contexts, research such as \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://research.trychroma.com/context-rot\",\"children\":[\"Chroma’s \",[\"$\",\"em\",null,{\"children\":\"context rot\"}],\" study\"]}],\" has shown that recall accuracy deteriorates as context length grows. This makes it challenging for LLMs to accurately interpret and reason over long, domain-specific documents such as financial reports or legal filings.\"]}],[\"$\",\"p\",null,{\"children\":[\"To mitigate this limitation, \",[\"$\",\"strong\",null,{\"children\":\"Retrieval-Augmented Generation (RAG)\"}],\" has emerged as the dominant solution. Instead of passing the entire document into the model, RAG retrieves and feeds only the most relevant text chunks based on the user’s query, thereby optimizing the effective context length. However, conventional \",[\"$\",\"strong\",null,{\"children\":\"vector-based RAG\"}],\" methods depend heavily on static semantic similarity and face several key limitations. To address these challenges, we introduce \",[\"$\",\"strong\",null,{\"children\":\"PageIndex\"}],\", a \",[\"$\",\"strong\",null,{\"children\":\"reasoning-based retrieval framework\"}],\" that enables LLMs to dynamically navigate document structures and infer which sections are genuinely relevant, rather than merely retrieving text that appears semantically similar.\"]}],[\"$\",\"h1\",null,{\"id\":\"the-limitations-of-vector-based-rag\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#the-limitations-of-vector-based-rag\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"icon icon-link\"}]}],[\"$\",\"strong\",null,{\"children\":\"The Limitations of Vector-Based RAG\"}]]}],[\"$\",\"p\",null,{\"children\":[\"Vector-based RAG relies on \",[\"$\",\"strong\",null,{\"children\":\"semantic embeddings\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"vector databases\"}],\" to identify relevant text chunks. In the \",[\"$\",\"strong\",null,{\"children\":\"preprocessing stage\"}],\", the document is first \",[\"$\",\"strong\",null,{\"children\":\"split\"}],\" into smaller chunks, then each chunk is \",[\"$\",\"strong\",null,{\"children\":\"embedded\"}],\" into a vector space using an embedding model, and the resulting vectors are \",[\"$\",\"strong\",null,{\"children\":\"stored\"}],\" in a vector database such as Chroma or Pinecone. During the \",[\"$\",\"strong\",null,{\"children\":\"query stage\"}],\", the user query is \",[\"$\",\"strong\",null,{\"children\":\"embedded\"}],\" using the same embedding model, the database is \",[\"$\",\"strong\",null,{\"children\":\"searched\"}],\" for semantically similar chunks, and the system \",[\"$\",\"strong\",null,{\"children\":\"retrieves\"}],\" the top-k results, which are then used to form the model’s input context. While simple and effective for short texts, vector-based RAG faces several major challenges:\"]}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Query and knowledge space mismatch\"}],\". Vector retrieval assumes that the \",[\"$\",\"em\",null,{\"children\":\"most semantically similar\"}],\" text to the query is also the \",[\"$\",\"em\",null,{\"children\":\"most relevant\"}],\". But this isn’t always true — queries often express \",[\"$\",\"em\",null,{\"children\":\"intent\"}],\", not \",[\"$\",\"em\",null,{\"children\":\"content\"}],\".\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Semantic similarity is not equivalent to relevance\"}],\". This is especially problematic in technical or legal documents, where many passages share near-identical semantics but differ in relevance.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Hard chunking breaks the semantic integration\"}],\". Documents are split into fixed-size chunks (e.g., 512 or 1000 tokens) for embedding. This “hard chunking” often cuts through sentences, paragraphs, or sections, fragmenting context.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Cannot integrate chat history\"}],\". Each query is treated independently. The retriever doesn’t know what’s been asked or answered before.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Hard to deal with in document reference\"}],\". Documents often contain references such as “see Appendix G” or “refer to Table 5.3.” Since these references don’t share semantic similarity with the referenced content, traditional RAG misses them unless additional preprocessing (like a knowledge graph) is performed.\"]}]}]]}],[\"$\",\"p\",null,{\"children\":[\"Because of these limitations, even advanced systems like \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://x.com/pashmerepat/status/1926717705660375463\",\"children\":[\"$\",\"strong\",null,{\"children\":\"Claude Code\"}]}],\" have moved away from traditional \",[\"$\",\"strong\",null,{\"children\":\"vector-based RAG\"}],\" for code retrieval, achieving superior precision and speed without relying on vector databases (see this \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://rlancemartin.github.io/2025/04/03/vibe-code/\",\"children\":\"blog post\"}],\"). We believe the same principle applies to \",[\"$\",\"strong\",null,{\"children\":\"document retrieval\"}],\": rather than depending on static embeddings and semantic similarity, LLMs can reason over a structured representation of a document — deciding \",[\"$\",\"em\",null,{\"children\":\"where\"}],\" to look next, not merely \",[\"$\",\"em\",null,{\"children\":\"what\"}],\" looks similar. To this end, we introduce \",[\"$\",\"strong\",null,{\"children\":\"PageIndex\"}],\", a reasoning-based RAG framework that overcomes the constraints of vector-based systems and brings the power of agentic retrieval to long-form, structured documents.\"]}],[\"$\",\"h1\",null,{\"id\":\"pageindex-reasoning-based-retrieval\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#pageindex-reasoning-based-retrieval\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"icon icon-link\"}]}],\"PageIndex: Reasoning-based Retrieval\"]}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"PageIndex's Reasoning-based RAG\"}],\" mimics how humans naturally navigate and extract information from long documents. Unlike traditional vector-based methods that rely on static semantic similarity, this approach uses a \",[\"$\",\"em\",null,{\"children\":\"dynamic, iterative reasoning process\"}],\" to actively decide where to look next based on the evolving context of the question.\"]}],[\"$\",\"p\",null,{\"children\":\"It follows the following iterative process:\"}],[\"$\",\"ol\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Read the Table of Contents (ToC)\"}],\". Understand the document's structure and identify sections that might be relevant.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Select a Section\"}],\". Choose the section most likely to contain useful information based on the question.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Extract Relevant Information\"}],\". Parse the selected section to gather any content that could help answer the question.\"]}]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Is the Information Sufficient?\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Yes →\"}],\" Proceed to \",[\"$\",\"strong\",null,{\"children\":\"Answer the Question\"}],\".\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"No →\"}],\" Return to \",[\"$\",\"strong\",null,{\"children\":\"Step 1\"}],\" and repeat the loop with another section.\"]}]]}]]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Answer the Question\"}],\". Once enough information is collected, generate a complete and well-supported answer.\"]}]}]]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"$L18\",null,{\"alt\":\"loop\",\"src\":\"/static/images/loop.png\",\"width\":\"1580\",\"height\":\"592\"}]}],[\"$\",\"p\",null,{\"children\":\"In this process, the ToC serves as a key index for the document, enabling the LLM to navigate and retrieve information efficiently. We discuss how to design an LLM-friendly ToC in the next section.\"}],[\"$\",\"h1\",null,{\"id\":\"table-of-contents--index-for-llms\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#table-of-contents--index-for-llms\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"icon icon-link\"}]}],[\"$\",\"strong\",null,{\"children\":\"\\\"Table of Contents\\\" Index for LLMs\"}]]}],[\"$\",\"p\",null,{\"children\":[\"We introduce a \",[\"$\",\"strong\",null,{\"children\":\"JSON-based hierarchical structure\"}],\" to represent a \",[\"$\",\"strong\",null,{\"children\":\"Table of Contents (ToC)\"}],\" for unstructured documents. The ToC acts as an \",[\"$\",\"strong\",null,{\"children\":\"index tree\"}],\" that organizes content into hierarchical nodes. Each node represents a logical section (e.g., chapter, paragraph, page) and may contain metadata, a description, and links to its sub-sections.\"]}],[\"$\",\"p\",null,{\"children\":\"This approach allows an LLM to:\"}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":\"Traverse structured content recursively.\"}],[\"$\",\"li\",null,{\"children\":[\"Retrieve targeted raw data by \",[\"$\",\"code\",null,{\"children\":\"node_id\"}],\".\"]}],[\"$\",\"li\",null,{\"children\":\"Associate contextual metadata (e.g., source type, topic, or semantic tags).\"}]]}],[\"$\",\"h3\",null,{\"id\":\"pageindex-tree-index-example-json-format\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#pageindex-tree-index-example-json-format\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"icon icon-link\"}]}],[\"$\",\"strong\",null,{\"children\":\"PageIndex Tree Index Example (JSON Format)\"}]]}],[\"$\",\"$L19\",null,{\"className\":\"language-python\",\"children\":[\"$\",\"code\",null,{\"className\":\"code-highlight language-python\",\"children\":[[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"Node \",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"{\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"  node_id\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\":\"}],\" string\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\"         \",[\"$\",\"span\",null,{\"className\":\"token operator\",\"children\":\"//\"}],\" Unique identifier \",[\"$\",\"span\",null,{\"className\":\"token keyword\",\"children\":\"for\"}],\" this node\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"  name\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\":\"}],\" string\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\"            \",[\"$\",\"span\",null,{\"className\":\"token operator\",\"children\":\"//\"}],\" Human\",[\"$\",\"span\",null,{\"className\":\"token operator\",\"children\":\"-\"}],\"readable label \",[\"$\",\"span\",null,{\"className\":\"token keyword\",\"children\":\"or\"}],\" title\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"  description\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\":\"}],\" string\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\"     \",[\"$\",\"span\",null,{\"className\":\"token operator\",\"children\":\"//\"}],\" Optional detailed explanation of the node\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"  metadata\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\":\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token builtin\",\"children\":\"object\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\"        \",[\"$\",\"span\",null,{\"className\":\"token operator\",\"children\":\"//\"}],\" Arbitrary key\",[\"$\",\"span\",null,{\"className\":\"token operator\",\"children\":\"-\"}],\"value pairs \",[\"$\",\"span\",null,{\"className\":\"token keyword\",\"children\":\"for\"}],\" context \",[\"$\",\"span\",null,{\"className\":\"token keyword\",\"children\":\"or\"}],\" attributes\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"  sub_nodes\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\":\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"[\"}],\"Node\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"]\"}],\"        \",[\"$\",\"span\",null,{\"className\":\"token operator\",\"children\":\"//\"}],\" Array of child nodes \",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"(\"}],\"recursive structure\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\")\"}],\"\\n\"]}],[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"}\"}],\"\\n\"]}]]}]}],[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Notes:\"}]}],[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\"node_id\"}],\" serves as a reference key to locate the corresponding raw data.\"]}],[\"$\",\"li\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\"sub_nodes\"}],\" field allows recursive nesting, forming a full ToC tree.\"]}],[\"$\",\"li\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\"metadata\"}],\" field can store semantic information such as document type, author, timestamp, or relevance scores.\"]}]]}],[\"$\",\"p\",null,{\"children\":[\"Each node in the ToC links directly to its corresponding \",[\"$\",\"strong\",null,{\"children\":\"raw content\"}],\" (e.g., text, images, tables):\"]}],[\"$\",\"$L19\",null,{\"className\":\"language-js\",\"children\":[\"$\",\"code\",null,{\"className\":\"code-highlight language-js\",\"children\":[\"$\",\"span\",null,{\"className\":\"code-line\",\"children\":[\"node_id → \",[\"$\",\"span\",null,{\"className\":\"token function\",\"children\":\"node_content\"}],\" \",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\"(\"}],\"raw content\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\" extracted text\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\" images\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\",\"}],\" etc\",[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\".\"}],[\"$\",\"span\",null,{\"className\":\"token punctuation\",\"children\":\")\"}],\"\\n\"]}]}]}],[\"$\",\"p\",null,{\"children\":[\"This mapping enables the LLM to \",[\"$\",\"strong\",null,{\"children\":\"select and retrieve\"}],\" specific nodes as needed, facilitating precise and context-aware information access.\"]}],[\"$\",\"p\",null,{\"children\":[\"Unlike a \",[\"$\",\"strong\",null,{\"children\":\"vector database\"}],\", which stores an external, static embeddings index, the \",[\"$\",\"strong\",null,{\"children\":\"JSON-based ToC index\"}],\" resides \",[\"$\",\"em\",null,{\"children\":\"within\"}],\" the LLM’s active reasoning context. We call this an \",[\"$\",\"strong\",null,{\"children\":\"in-context index\"}],\" — a structure the model can directly reference, navigate, and reason over during inference. By integrating the index into the model’s context window, the LLM can dynamically decide \",[\"$\",\"em\",null,{\"children\":\"where to look next\"}],\" rather than depending solely on precomputed similarity scores. This enables in-context \",[\"$\",\"strong\",null,{\"children\":\"reasoning-driven retrieval\"}],\", effectively addressing many of the constraints inherent in traditional vector-based RAG systems.\"]}],[\"$\",\"h1\",null,{\"id\":\"overcoming-the-limitations\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#overcoming-the-limitations\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"icon icon-link\"}]}],\"Overcoming the Limitations\"]}],[\"$\",\"h3\",null,{\"id\":\"1-queryknowledge-space-mismatch\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#1-queryknowledge-space-mismatch\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"icon icon-link\"}]}],[\"$\",\"strong\",null,{\"children\":\"1. Query–Knowledge Space Mismatch\"}]]}],[\"$\",\"p\",null,{\"children\":[\"Instead of relying solely on embedding overlap, the LLM \",[\"$\",\"strong\",null,{\"children\":\"uses reasoning to infer which section is likely to contain the answer\"}],\". It can “think” about document structure — e.g., \",[\"$\",\"em\",null,{\"children\":\"“Debt trends are usually in the financial summary section or Appendix G — let’s look there.”\"}],\" This dynamic inference bridges the gap between \",[\"$\",\"em\",null,{\"children\":\"query meaning\"}],\" and \",[\"$\",\"em\",null,{\"children\":\"information location\"}],\".\"]}],[\"$\",\"h3\",null,{\"id\":\"2-semantic-similarity--true-relevance\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#2-semantic-similarity--true-relevance\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"icon icon-link\"}]}],[\"$\",\"strong\",null,{\"children\":\"2. Semantic Similarity ≠ True Relevance\"}]]}],[\"$\",\"p\",null,{\"children\":[\"Reasoning-based retrieval emphasizes \",[\"$\",\"strong\",null,{\"children\":\"contextual relevance\"}],\", not just similarity. The model reads the Table of Contents (ToC) or PageIndex structure, interprets the query’s intent, and \",[\"$\",\"strong\",null,{\"children\":\"navigates\"}],\" to sections that actually contain the answer — even if their language is different. This mirrors how humans find answers: by \",[\"$\",\"em\",null,{\"children\":\"understanding\"}],\" the question, not just matching words.\"]}],[\"$\",\"h3\",null,{\"id\":\"3-hard-chunking-breaks-semantic-integration\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#3-hard-chunking-breaks-semantic-integration\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"icon icon-link\"}]}],[\"$\",\"strong\",null,{\"children\":\"3. Hard Chunking Breaks Semantic Integration\"}]]}],[\"$\",\"p\",null,{\"children\":[\"Rather than chunking arbitrarily, reasoning-based RAG retrieves \",[\"$\",\"strong\",null,{\"children\":\"semantically coherent sections\"}],\" (e.g., full pages, sections, or chapters). If the model detects that a section is incomplete, it \",[\"$\",\"strong\",null,{\"children\":\"iteratively fetches neighboring sections\"}],\" (e.g., next page or sub-node) until context is sufficient. This preserves logical continuity and minimizes hallucination.\"]}],[\"$\",\"h3\",null,{\"id\":\"4-inability-to-integrate-chat-history\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#4-inability-to-integrate-chat-history\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"icon icon-link\"}]}],[\"$\",\"strong\",null,{\"children\":\"4. Inability to Integrate Chat History\"}]]}],[\"$\",\"p\",null,{\"children\":[\"Retrieval is \",[\"$\",\"strong\",null,{\"children\":\"context-aware\"}],\" — the model uses prior conversation history to refine its understanding of the current question. For instance, if the user previously asked about “financial assets,” and now asks, “What about liabilities?”, the retriever knows to explore \",[\"$\",\"em\",null,{\"children\":\"the same report section\"}],\" under liabilities. This enables coherent, multi-step exploration across multiple turns.\"]}],[\"$\",\"h3\",null,{\"id\":\"5-poor-handling-of-in-document-references\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#5-poor-handling-of-in-document-references\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"icon icon-link\"}]}],[\"$\",\"strong\",null,{\"children\":\"5. Poor Handling of In-Document References\"}]]}],[\"$\",\"p\",null,{\"children\":[\"By leveraging the \",[\"$\",\"strong\",null,{\"children\":\"PageIndex\"}],\" or \",[\"$\",\"strong\",null,{\"children\":\"ToC-based hierarchical structure\"}],\", reasoning-based retrieval can \",[\"$\",\"em\",null,{\"children\":\"follow references like a human reader\"}],\". When it encounters a phrase like \",[\"$\",\"em\",null,{\"children\":\"“see Appendix G”\"}],\", the LLM navigates the index tree to that section and retrieves the relevant data. This allows accurate cross-referencing without manual link-building.\"]}],[\"$\",\"p\",null,{\"children\":[\"In the \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://claude.ai/share/b29bdfac-f22a-478d-ac1e-00e604c4f3fd\",\"children\":\"PageIndex MCP example\"}],\", the query asked for the \",[\"$\",\"em\",null,{\"children\":\"total value of deferred assets\"}],\". The main section (pages 75–82) only reported the \",[\"$\",\"em\",null,{\"children\":\"increase\"}],\" in value, not the total. On page 77, the text read:\"]}],[\"$\",\"blockquote\",null,{\"children\":[\"$\",\"p\",null,{\"children\":\"Table 5.3 summarizes the income, expenses, and distributions of the Reserve Banks for 2023 and 2022. Appendix G of this report, ‘Statistical Tables,’ provides more detailed information…\"}]}],[\"$\",\"p\",null,{\"children\":\"The reasoning-based retriever followed this cue to Appendix G, found the correct table, and returned the total deferred asset value — a task the vector-based retriever would likely fail.\"}],[\"$\",\"h2\",null,{\"id\":\"summary-vector-vs-reasoning-based-rag\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#summary-vector-vs-reasoning-based-rag\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"icon icon-link\"}]}],[\"$\",\"strong\",null,{\"children\":\"Summary: Vector vs. Reasoning-Based RAG\"}]]}],[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Limitation\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Vector-Based RAG\"}]}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Reasoning-Based RAG\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"1. Query–Knowledge Mismatch\"}]}],[\"$\",\"td\",null,{\"children\":\"Matches surface-level similarity; often misses true context\"}],[\"$\",\"td\",null,{\"children\":\"Uses inference to identify the most relevant document sections\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"2. Semantic ≠ Relevance\"}]}],[\"$\",\"td\",null,{\"children\":\"Retrieves semantically similar but irrelevant chunks\"}],[\"$\",\"td\",null,{\"children\":\"Retrieves contextually relevant information\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"3. Hard Chunking\"}]}],[\"$\",\"td\",null,{\"children\":\"Fixed-length chunks fragment meaning\"}],[\"$\",\"td\",null,{\"children\":\"Retrieves coherent sections dynamically\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"4. No Chat Context\"}]}],[\"$\",\"td\",null,{\"children\":\"Each query is isolated\"}],[\"$\",\"td\",null,{\"children\":\"Multi-turn reasoning considers prior context\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"5. Cross-References\"}]}],[\"$\",\"td\",null,{\"children\":\"Fails to follow internal document links\"}],[\"$\",\"td\",null,{\"children\":\"Follows in-text references via ToC/PageIndex reasoning\"}]]}]]}]]}],[\"$\",\"h1\",null,{\"id\":\"conclusion\",\"children\":[[\"$\",\"a\",null,{\"href\":\"#conclusion\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"icon icon-link\"}]}],[\"$\",\"strong\",null,{\"children\":\"Conclusion\"}]]}],[\"$\",\"p\",null,{\"children\":[\"Vector-based RAG \",[\"$\",\"strong\",null,{\"children\":\"searches\"}],\" for similar text whereas reasoning-based RAG \",[\"$\",\"strong\",null,{\"children\":\"thinks\"}],\" about \",[\"$\",\"em\",null,{\"children\":\"where\"}],\" to look and \",[\"$\",\"em\",null,{\"children\":\"why\"}],\". By combining structured document representations (like ToC Trees) with iterative reasoning, reasoning-based RAG enables LLMs to retrieve \",[\"$\",\"strong\",null,{\"children\":\"the relevant information\"}],\", not just \",[\"$\",\"strong\",null,{\"children\":\"similar information\"}],\" — paving the way for a new generation of intelligent document understanding systems.\"]}],[\"$\",\"p\",null,{\"children\":[\"Try \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://chat.pageindex.ai\",\"children\":\"PageIndex\"}],\" Now.\"]}]]}]}],[\"$\",\"footer\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"flex justify-center pt-4 xl:pt-8\",\"children\":[\"$\",\"$L13\",null,{\"href\":\"/blog\",\"className\":\"text-gray-500 hover:text-gray-700 dark:text-gray-400 dark:hover:text-gray-300 flex items-center gap-2 hover:underline\",\"children\":[\"$\",\"span\",null,{\"children\":\"Back to Blog\"}]}]}]}]]}]]}]}],[\"$\",\"$1a\",null,{\"fallback\":null,\"children\":[\"$\",\"$L1b\",null,{\"children\":[\"$\",\"$L1c\",null,{}]}]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"PageIndex: Next-Generation Vectorless, Reasoning-based RAG | PageIndex\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"We present PageIndex, a reasoning-based RAG system that simulates how human experts navigate and extract knowledge from long documents through tree search.\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"4\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"5\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"6\",{\"rel\":\"canonical\",\"href\":\"https://pageindex.ai/blog/pageindex-intro\"}],[\"$\",\"link\",\"7\",{\"rel\":\"alternate\",\"type\":\"application/rss+xml\",\"href\":\"https://pageindex.ai/feed.xml\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:title\",\"content\":\"PageIndex: Next-Generation Vectorless, Reasoning-based RAG\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:description\",\"content\":\"We present PageIndex, a reasoning-based RAG system that simulates how human experts navigate and extract knowledge from long documents through tree search.\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:url\",\"content\":\"https://pageindex.ai/blog/pageindex-intro\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:site_name\",\"content\":\"PageIndex\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:image\",\"content\":\"https://pageindex.ai/static/images/blog/blog_pageindex.jpg\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"15\",{\"property\":\"article:published_time\",\"content\":\"2025-09-19T00:00:00.000Z\"}],[\"$\",\"meta\",\"16\",{\"property\":\"article:modified_time\",\"content\":\"2025-09-19T00:00:00.000Z\"}],[\"$\",\"meta\",\"17\",{\"property\":\"article:author\",\"content\":\"Mingtian Zhang\"}],[\"$\",\"meta\",\"18\",{\"property\":\"article:author\",\"content\":\"Yu Tang\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"20\",{\"name\":\"twitter:title\",\"content\":\"PageIndex: Next-Generation Vectorless, Reasoning-based RAG\"}],[\"$\",\"meta\",\"21\",{\"name\":\"twitter:description\",\"content\":\"We present PageIndex, a reasoning-based RAG system that simulates how human experts navigate and extract knowledge from long documents through tree search.\"}],[\"$\",\"meta\",\"22\",{\"name\":\"twitter:image\",\"content\":\"https://pageindex.ai/static/images/blog/blog_pageindex.jpg\"}],[\"$\",\"meta\",\"23\",{\"name\":\"next-size-adjust\"}]]\n"])</script></body></html>